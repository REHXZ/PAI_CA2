{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yongj\\AppData\\Local\\Temp\\ipykernel_3080\\851814882.py:8: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.\n",
      "  from pandas_profiling import ProfileReport\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import warnings\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import mlflow\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import itertools\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/merged_data.csv')\n",
    "\n",
    "merged_df=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dtypes(df):\n",
    "    # Convert 'order_value' and 'refund_value' to float16 for memory efficiency\n",
    "    df['order_value'] = df['order_value'].astype('float32')\n",
    "    df['refund_value'] = df['refund_value'].astype('float32')\n",
    "    \n",
    "    # Convert 'num_items_ordered' to uint8 after rounding\n",
    "    df['num_items_ordered'] = df['num_items_ordered'].astype(float).round().astype('uint8')\n",
    "    \n",
    "    # Convert 'order_date' and 'first_order_datetime' to datetime\n",
    "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "    df['first_order_datetime'] = pd.to_datetime(df['first_order_datetime'])\n",
    "    \n",
    "    # Convert categorical columns to category dtype for efficiency\n",
    "    df[['country_code', 'collect_type', 'payment_method']] = df[['country_code', 'collect_type', 'payment_method']].astype('category')\n",
    "    \n",
    "    # Convert numerical columns (those that represent counts or numeric features) to uint16\n",
    "    df[['num_orders_last_50days', 'num_cancelled_orders_last_50days', 'num_refund_orders_last_50days']] = df[['num_orders_last_50days', 'num_cancelled_orders_last_50days', 'num_refund_orders_last_50days']].astype('uint16')\n",
    "    \n",
    "    # Convert 'num_associated_customers' to uint8 for efficient memory usage\n",
    "    df['num_associated_customers'] = df['num_associated_customers'].astype('uint8')\n",
    "    \n",
    "    # Convert 'total_payment_last_50days' to float16 for memory efficiency\n",
    "    df['total_payment_last_50days'] = df['total_payment_last_50days'].astype('float32')\n",
    "    \n",
    "    # Convert 'mobile_verified' and 'is_fraud' columns to boolean (mapping string values)\n",
    "    # df['mobile_verified'] = df['mobile_verified'].map({'True': True, 'False': False})\n",
    "    # df['is_fraud'] = df['is_fraud'].map({'1': True, '0': False})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_payment_methods(payment_method):\n",
    "    # Credit Card and Related Gateways\n",
    "    if payment_method in ['GenericCreditCard', 'CybersourceCreditCard', 'CybersourceApplePay', 'CreditCard']:\n",
    "        return 'CreditCard'\n",
    "    \n",
    "    # Digital Wallets\n",
    "    elif payment_method in ['GCash', 'AFbKash', 'JazzCashWallet', 'AFTrueMoney', 'AdyenBoost', 'AdyenMolpay',\n",
    "                            'AFTNG', 'AdyenHPPBoost', 'AdyenHPPMolpay', 'PayPal', 'AFGCash', 'AccountBalance']:\n",
    "        return 'DigitalWallet'\n",
    "    \n",
    "    # Bank Transfers and Direct Debit\n",
    "    elif payment_method in ['XenditDirectDebit', 'RazerOnlineBanking']:\n",
    "        return 'BankTransfer'\n",
    "    \n",
    "    # PayOnDelivery\n",
    "    elif payment_method in ['Invoice', 'PayOnDelivery']:\n",
    "        return 'PaymentOnDelivery'\n",
    "    \n",
    "    # Default case for unrecognized methods\n",
    "    else:\n",
    "        return 'Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_since_first_order(data, order_date_column, first_order_column):\n",
    "    # Create a feature for the number of days since the first order\n",
    "    data['days_since_first_order'] = (data[order_date_column] - data[first_order_column]).dt.days\n",
    "    data.drop([first_order_column], axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def transform_datetime(data, column):\n",
    "    # Handle Datetime columns\n",
    "    data[column + '_day_of_week'] = data[column].dt.dayofweek\n",
    "    data[column + '_day'] = data[column].dt.day\n",
    "    data[column + '_month'] = data[column].dt.month\n",
    "    data[column + '_year'] = data[column].dt.year\n",
    "    data.drop([column], axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "def date_transformations(data):\n",
    "    data = days_since_first_order(data, 'order_date', 'first_order_datetime')\n",
    "    data = transform_datetime(data, 'order_date')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = convert_dtypes(merged_df)\n",
    "merged_df['payment_method'] = merged_df['payment_method'].apply(group_payment_methods)\n",
    "merged_df = date_transformations(merged_df)\n",
    "merged_df.drop(columns=['order_id', 'customer_id'], inplace=True)\n",
    "X=merged_df.drop(columns=['is_fraud'])\n",
    "y=merged_df['is_fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ExperimentTrackers import PhaseOneExperimentTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PhaseOneExperimentTracker(BaseExperimentTracker):\n",
    "#     def run_experiments(self, experiment_combinations, X_train, y_train, X_test, y_test, numeric_columns, categorical_cols):\n",
    "#         \"\"\"Run experiments for Phase 1.\"\"\"\n",
    "        \n",
    "#         for config in experiment_combinations:\n",
    "#             run_id = self._generate_run_id(config)\n",
    "            \n",
    "#             # Skip if this configuration has already been run\n",
    "#             if run_id in self.completed_runs:\n",
    "#                 print(f\"Skipping completed run: {run_id}\")\n",
    "#                 continue\n",
    "            \n",
    "#             print(f\"Starting run: {run_id}\")\n",
    "            \n",
    "#             try:\n",
    "#                 with mlflow.start_run(run_name=run_id):\n",
    "#                     # Add descriptive run tags\n",
    "#                     mlflow.set_tag(\"model_type\", config['models']['name'])\n",
    "#                     mlflow.set_tag(\"scaler_type\", config['scaler'].__class__.__name__)\n",
    "#                     mlflow.set_tag(\"encoding_applied\", str(config['encode']['apply']))\n",
    "#                     mlflow.set_tag(\"dataset\", \"X_train\")\n",
    "\n",
    "\n",
    "#                     # Build preprocessing steps\n",
    "#                     transformers = []\n",
    "#                     if config['scaler']:\n",
    "#                         transformers.append(('scaler', config['scaler'], numeric_columns))\n",
    "                    \n",
    "#                     if config['encode']['apply']:\n",
    "#                         transformers.append(('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=True), categorical_cols))\n",
    "#                     else:\n",
    "#                         transformers.append(('drop_categorical', 'drop', categorical_cols))\n",
    "                    \n",
    "#                     preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "#                     pipeline = Pipeline(steps=[\n",
    "#                         ('preprocessor', preprocessor),\n",
    "#                         ('model', config['models']['instance']),\n",
    "#                     ])\n",
    "                    \n",
    "#                     # Train the pipeline\n",
    "#                     pipeline.fit(X_train, y_train.to_numpy().ravel())\n",
    "#                     predictions = pipeline.predict(X_test)\n",
    "#                     probabilities = pipeline.predict_proba(X_test)[:, 1]  # For metrics requiring probabilities\n",
    "\n",
    "#                     # Evaluate metrics\n",
    "#                     metrics = self.evaluate_metrics(y_test, predictions, probabilities)\n",
    "                    \n",
    "#                     # Log parameters, metrics, and model\n",
    "#                     mlflow.log_params(config)\n",
    "#                     mlflow.log_metrics(metrics)\n",
    "#                     mlflow.sklearn.log_model(pipeline, \"model\", input_example=X_train.iloc[0:1])\n",
    "                    \n",
    "#                     # Mark this run as completed\n",
    "#                     self.completed_runs.add(run_id)\n",
    "#                     self._save_checkpoint()\n",
    "                    \n",
    "#                     print(f\"Completed run: {run_id}\")\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error in run {run_id}: {str(e)}\")\n",
    "# #               continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary models\n",
    "# Import XGBoost and LightGBM Classifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Define the search space\n",
    "search_space = search_space = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(), None],\n",
    "    'encode': [{'apply': True, 'columns': ['categorical_col']}, {'apply': False}],\n",
    "    'models': [\n",
    "        # {'name': 'LogisticRegression', 'instance': LogisticRegression()},\n",
    "        # {'name': 'RandomForest', 'instance': RandomForestClassifier()},\n",
    "        # {'name': 'XGBoost', 'instance': XGBClassifier()},\n",
    "        # {'name': 'LightGBM', 'instance': LGBMClassifier()},\n",
    "        {'name': 'KNeighbors', 'instance': KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')},\n",
    "        # {'name': 'GaussianNB', 'instance': GaussianNB()},\n",
    "        # {'name': 'SVM_Linear', 'instance': LinearSVC()},\n",
    "        # {'name': 'DecisionTree', 'instance': DecisionTreeClassifier()},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate all combinations of the search space\n",
    "keys, values = zip(*search_space.items())\n",
    "experiment_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "categorical_cols = ['payment_method', 'country_code', 'collect_type']\n",
    "numeric_columns = ['order_value', 'refund_value', 'num_items_ordered', 'num_orders_last_50days', 'num_cancelled_orders_last_50days', 'num_refund_orders_last_50days', 'num_associated_customers', 'total_payment_last_50days', 'days_since_first_order', 'order_date_day_of_week', 'order_date_day', 'order_date_month', 'order_date_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run: KN_Standard_Enc\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tracker\n",
    "tracker = PhaseOneExperimentTracker(\"Testing_Preprocessing_Methods\")\n",
    "\n",
    "# Load checkpoint file\n",
    "tracker.completed_runs\n",
    "\n",
    "# Run experiments with checkpointing\n",
    "tracker.run_experiments(\n",
    "    experiment_combinations=experiment_combinations,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    numeric_columns=numeric_columns,\n",
    "    categorical_cols=categorical_cols\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PAIenv (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
