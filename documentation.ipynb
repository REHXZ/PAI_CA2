{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fraud Detection Project - CA2**\n",
    "\n",
    "## **1. Introduction**\n",
    "\n",
    "### Project Overview\n",
    "This project is the second phase (CA2) of a comprehensive fraud detection system for **SP-Buy**, an e-commerce platform. Building on the work done in **CA1**, which involved data ingestion, exploratory data analysis (EDA), and the creation of an interactive dashboard, this phase focuses on developing a machine learning model to predict fraudulent orders. The goal is to deploy this model in a user-friendly application that the operations team can use to proactively identify and mitigate fraud.\n",
    "\n",
    "### Problem Statement\n",
    "Fraudulent activities pose a significant threat to e-commerce platforms, leading to financial losses, reputational damage, and customer dissatisfaction. Detecting fraud in real-time is challenging due to the imbalanced nature of the data (fraud cases are rare compared to legitimate transactions) and the evolving tactics of fraudsters. In this project, we aim to build a robust machine learning model that can accurately identify fraudulent orders based on historical data.\n",
    "\n",
    "### CA1 1 Recap\n",
    "In **CA1 1**, we performed extensive exploratory data analysis (EDA) and data cleaning on the provided datasets:\n",
    "- **Customer Features**: Information about customers, such as their order history and verification status.\n",
    "- **Order Features**: Details about each order, including payment method and order value.\n",
    "- **Labels**: Fraud labels indicating whether an order was fraudulent.\n",
    "\n",
    "The cleaned and preprocessed data was used to create an interactive dashboard for monitoring fraud trends and patterns. This dashboard provided valuable insights into the dataset, enabling stakeholders to understand the nature of fraud on the platform.\n",
    "\n",
    "### CA2 Objectives\n",
    "In **CA2**, we shift our focus to **model development** and **deployment**. The key objectives are:\n",
    "1. **Advanced Data Analysis**:\n",
    "   - Perform additional EDA to identify feature importance and detect outliers, which are critical for model creation.\n",
    "2. **Model Development**:\n",
    "   - Train, evaluate, and optimize machine learning models to predict fraudulent orders.\n",
    "3. **Experiment Tracking**:\n",
    "   - Use **MLflow** to track experiments, log parameters, and compare model performance.\n",
    "4. **Deployment**:\n",
    "   - Develop a **Tkinter-based GUI application** to allow the operations team to make predictions on new orders.\n",
    "5. **Automation**:\n",
    "   - Design the workflow to be modular and scalable, enabling future integration with **Airflow** for automation.\n",
    "\n",
    "### Key Challenges\n",
    "- **Imbalanced Data**: Fraud cases are rare, making it challenging to train a model that can accurately detect them.\n",
    "- **Feature Engineering**: Identifying and creating meaningful features that improve model performance.\n",
    "- **Deciding how to proceed with experiments**: Balancing the need for thorough experimentation with time constraints was a key challenge. We addressed this by prioritizing techniques likely to have the most impact (e.g., handling imbalanced data, feature engineering).\n",
    "\n",
    "### Structure of the Report\n",
    "This report documents the entire process of **CA2**, from advanced data analysis and model development to deployment and automation. The following sections provide a detailed breakdown of each step:\n",
    "- **Exploratory Data Analysis (EDA)**: Additional analysis focusing on feature importance and outlier detection.\n",
    "- **Feature Engineering**: Creation of new features and their impact on model performance.\n",
    "- **Data Preprocessing**: Techniques used to prepare the data for modeling.\n",
    "- **Model Training and Evaluation**: Development and comparison of machine learning models.\n",
    "- **Deployment**: Development of the GUI application and integration of the final model.\n",
    "- **Conclusion**: Summary of findings, challenges, and future work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **2. Preparing the Dataset and Libraries**\n",
    "\n",
    "---\n",
    "\n",
    "#### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import warnings\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import mlflow\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# ## SQL Server Name and Database Name\n",
    "# server = 'yj\\SQLEXPRESS'\n",
    "# database = 'PAI_CA1'\n",
    "\n",
    "# ## Create a connection to the SQL Server\n",
    "# engine = create_engine('mssql+pyodbc://{}/{}?driver=ODBC Driver 17 for SQL Server'.format(server, database))\n",
    "\n",
    "# customer_df = pd.read_sql('SELECT * FROM [dbo].[clean-data-customer_v1.0]', engine)\n",
    "# order_df = pd.read_sql('SELECT * FROM [dbo].[clean-data-order_v1.0]', engine)\n",
    "# label_df = pd.read_sql('SELECT * FROM [dbo].[clean-data-label_v1.0]', engine)\n",
    "\n",
    "\n",
    "# # Drop index column\n",
    "# customer_df.drop(columns=['index'], inplace=True)\n",
    "# order_df.drop(columns=['index'], inplace=True)\n",
    "# label_df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "# # merge the data\n",
    "# merged_df = pd.merge(label_df, customer_df, on=['customer_id', 'country_code'], how='inner')\n",
    "# merged_df = pd.merge(merged_df, order_df, on=['order_id', 'country_code'], how='inner')\n",
    "\n",
    "# merged_df.to_csv('merged_data.csv', index=False)\n",
    "\n",
    "data = pd.read_csv('./data/merged_data.csv')\n",
    "\n",
    "merged_df=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>mobile_verified</th>\n",
       "      <th>num_orders_last_50days</th>\n",
       "      <th>num_cancelled_orders_last_50days</th>\n",
       "      <th>num_refund_orders_last_50days</th>\n",
       "      <th>total_payment_last_50days</th>\n",
       "      <th>num_associated_customers</th>\n",
       "      <th>first_order_datetime</th>\n",
       "      <th>collect_type</th>\n",
       "      <th>payment_method</th>\n",
       "      <th>order_value</th>\n",
       "      <th>num_items_ordered</th>\n",
       "      <th>refund_value</th>\n",
       "      <th>order_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BD</td>\n",
       "      <td>w2lx-myz3</td>\n",
       "      <td>bdpr8uva</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-08-13 03:53:52</td>\n",
       "      <td>delivery</td>\n",
       "      <td>PayOnDelivery</td>\n",
       "      <td>8.664062</td>\n",
       "      <td>9</td>\n",
       "      <td>0.870117</td>\n",
       "      <td>2023-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BD</td>\n",
       "      <td>ta7z-r91q</td>\n",
       "      <td>bd59rlzo</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>228.042468</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-05-08 14:29:19</td>\n",
       "      <td>delivery</td>\n",
       "      <td>CreditCard</td>\n",
       "      <td>21.859375</td>\n",
       "      <td>4</td>\n",
       "      <td>2.279297</td>\n",
       "      <td>2023-02-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BD</td>\n",
       "      <td>t5af-wgb2</td>\n",
       "      <td>bd6zhjvq</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>45.674685</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-08-25 07:47:00</td>\n",
       "      <td>delivery</td>\n",
       "      <td>AFbKash</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.349609</td>\n",
       "      <td>2023-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BD</td>\n",
       "      <td>sibu-9lm4</td>\n",
       "      <td>bd4fv4rb</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>279.805231</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-12-06 13:53:22</td>\n",
       "      <td>delivery</td>\n",
       "      <td>CreditCard</td>\n",
       "      <td>4.535156</td>\n",
       "      <td>5</td>\n",
       "      <td>0.150024</td>\n",
       "      <td>2023-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BD</td>\n",
       "      <td>we61-omtr</td>\n",
       "      <td>bdzeepq7</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>107.067610</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-07-04 11:45:39</td>\n",
       "      <td>delivery</td>\n",
       "      <td>PayOnDelivery</td>\n",
       "      <td>3.011719</td>\n",
       "      <td>1</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>2023-01-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_code   order_id customer_id  is_fraud  mobile_verified  \\\n",
       "0           BD  w2lx-myz3    bdpr8uva         0             True   \n",
       "1           BD  ta7z-r91q    bd59rlzo         0             True   \n",
       "2           BD  t5af-wgb2    bd6zhjvq         0             True   \n",
       "3           BD  sibu-9lm4    bd4fv4rb         0             True   \n",
       "4           BD  we61-omtr    bdzeepq7         0             True   \n",
       "\n",
       "   num_orders_last_50days  num_cancelled_orders_last_50days  \\\n",
       "0                       0                                 0   \n",
       "1                       7                                 0   \n",
       "2                       4                                 1   \n",
       "3                      19                                 0   \n",
       "4                      30                                 6   \n",
       "\n",
       "   num_refund_orders_last_50days  total_payment_last_50days  \\\n",
       "0                              0                   0.000000   \n",
       "1                              0                 228.042468   \n",
       "2                              0                  45.674685   \n",
       "3                              3                 279.805231   \n",
       "4                              4                 107.067610   \n",
       "\n",
       "   num_associated_customers first_order_datetime collect_type payment_method  \\\n",
       "0                         3  2022-08-13 03:53:52     delivery  PayOnDelivery   \n",
       "1                         4  2022-05-08 14:29:19     delivery     CreditCard   \n",
       "2                         2  2021-08-25 07:47:00     delivery        AFbKash   \n",
       "3                         5  2021-12-06 13:53:22     delivery     CreditCard   \n",
       "4                         5  2020-07-04 11:45:39     delivery  PayOnDelivery   \n",
       "\n",
       "   order_value  num_items_ordered  refund_value  order_date  \n",
       "0     8.664062                  9      0.870117  2023-04-08  \n",
       "1    21.859375                  4      2.279297  2023-02-13  \n",
       "2     7.125000                  1      2.349609  2023-03-06  \n",
       "3     4.535156                  5      0.150024  2023-01-29  \n",
       "4     3.011719                  1      3.750000  2023-01-16  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to appropriate dtypes after importing daset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before conversion: 140.349328 MB\n",
      "Memory usage after conversion: 144.876688 MB\n"
     ]
    }
   ],
   "source": [
    "def convert_dtypes(df):\n",
    "    # Convert 'order_value' and 'refund_value' to float16 for memory efficiency\n",
    "    df['order_value'] = df['order_value'].astype('float32')\n",
    "    df['refund_value'] = df['refund_value'].astype('float32')\n",
    "    \n",
    "    # Convert 'num_items_ordered' to uint8 after rounding\n",
    "    df['num_items_ordered'] = df['num_items_ordered'].astype(float).round().astype('uint8')\n",
    "    \n",
    "    # Convert 'order_date' and 'first_order_datetime' to datetime\n",
    "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "    df['first_order_datetime'] = pd.to_datetime(df['first_order_datetime'])\n",
    "    \n",
    "    # Convert categorical columns to category dtype for efficiency\n",
    "    df[['country_code', 'collect_type', 'payment_method']] = df[['country_code', 'collect_type', 'payment_method']].astype('category')\n",
    "    \n",
    "    # Convert numerical columns (those that represent counts or numeric features) to uint16\n",
    "    df[['num_orders_last_50days', 'num_cancelled_orders_last_50days', 'num_refund_orders_last_50days']] = df[['num_orders_last_50days', 'num_cancelled_orders_last_50days', 'num_refund_orders_last_50days']].astype('uint16')\n",
    "    \n",
    "    # Convert 'num_associated_customers' to uint8 for efficient memory usage\n",
    "    df['num_associated_customers'] = df['num_associated_customers'].astype('uint8')\n",
    "    \n",
    "    # Convert 'total_payment_last_50days' to float16 for memory efficiency\n",
    "    df['total_payment_last_50days'] = df['total_payment_last_50days'].astype('float32')\n",
    "    \n",
    "    # Convert 'mobile_verified' and 'is_fraud' columns to boolean (mapping string values)\n",
    "    # df['mobile_verified'] = df['mobile_verified'].map({'True': True, 'False': False})\n",
    "    # df['is_fraud'] = df['is_fraud'].map({'1': True, '0': False})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Mermory before\n",
    "print(f'Memory usage before conversion: {merged_df.memory_usage().sum() / 1e6} MB')\n",
    "merged_df = convert_dtypes(merged_df)\n",
    "# Mermory after\n",
    "print(f'Memory usage after conversion: {merged_df.memory_usage().sum() / 1e6} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country_code                              category\n",
       "order_id                                    object\n",
       "customer_id                                 object\n",
       "is_fraud                                     int64\n",
       "mobile_verified                               bool\n",
       "num_orders_last_50days                      uint16\n",
       "num_cancelled_orders_last_50days            uint16\n",
       "num_refund_orders_last_50days               uint16\n",
       "total_payment_last_50days                  float16\n",
       "num_associated_customers                     uint8\n",
       "first_order_datetime                datetime64[ns]\n",
       "collect_type                              category\n",
       "payment_method                            category\n",
       "order_value                                float16\n",
       "num_items_ordered                            uint8\n",
       "refund_value                               float16\n",
       "order_date                          datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **3. Exploratory Data Analysis (EDA)**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Overview\n",
    "In **CA1**, we performed initial exploratory data analysis (EDA) to understand the structure and distribution of the dataset. This included:\n",
    "\n",
    "- Merging the datasets (customer-features.csv, order-features.csv, labels.csv).\n",
    "\n",
    "- Cleaning the data (e.g., handling missing values, removing duplicates).\n",
    "\n",
    "- Visualizing the distribution of fraud vs. non-fraud cases.\n",
    "\n",
    "In **CA2**, we focus on additional EDA tasks that are essential for model creation:\n",
    "- Imbalanced Data: Check how imbalanced is the dataset\n",
    "\n",
    "- Feature Importance: Identifying which features have the most impact on predicting fraud.\n",
    "\n",
    "- Outlier Detection: Detecting and handling outliers that could skew model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d65005a2c354364a02f135cf24dba9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebc55b845164432b756eaf4777942f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3953a989e400441599d6177cbf3d3129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c7e2466cc4474982e504bc39223625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## \n",
    "merged_profile = merged_df.profile_report(title='Merged Data Profiling Report', explorative=True)\n",
    "merged_profile.to_file('merged_data_profiling_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **4. Model Workflow**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 1: Baseline Experiments\n",
    "Preprocessing: Basic encoding (e.g., one-hot encoding) and scaling.\n",
    "\n",
    "Models: Logistic Regression, Random Forest, XGBoost.\n",
    "\n",
    "Goal: Establish baseline performance metrics.\n",
    "\n",
    "Phase 2: Advanced Preprocessing\n",
    "Introduce techniques like SMOTE, RUS, ROS for handling imbalanced data.\n",
    "\n",
    "Experiment with different encoding strategies (e.g., target encoding, frequency encoding).\n",
    "\n",
    "Goal: Identify which preprocessing steps improve performance.\n",
    "\n",
    "Phase 3: Feature Engineering and Selection\n",
    "Create new features (e.g., time-based features, aggregated customer behavior).\n",
    "\n",
    "Apply feature selection techniques (e.g., PCA, feature importance from tree-based models).\n",
    "\n",
    "Goal: Optimize the feature set for the best-performing models.\n",
    "\n",
    "Phase 4: Hyperparameter Tuning\n",
    "Use Grid Search or Bayesian Optimization to fine-tune hyperparameters for the best-performing models.\n",
    "\n",
    "Goal: Maximize model performance.\n",
    "\n",
    "Phase 5: Final Pipeline\n",
    "Combine the best preprocessing steps, feature engineering techniques, and models into a single pipeline.\n",
    "\n",
    "Deploy the pipeline in your GUI application.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 1: We defined the key metrics we will be using to determine the we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PAIenv (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
