{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "from ExperimentTracker2 import PhaseOneExperimentTracker\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "# Initialize Dask client for parallel processing\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load large dataset with Dask\n",
    "df = dd.read_csv(\"data/merged_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types for memory efficiency\n",
    "def convert_dtypes(df):\n",
    "    df['order_value'] = df['order_value'].astype('float32')\n",
    "    df['refund_value'] = df['refund_value'].astype('float32')\n",
    "    df['num_items_ordered'] = df['num_items_ordered'].astype(float).round().astype('uint8')\n",
    "    df['order_date'] = dd.to_datetime(df['order_date'])\n",
    "    df['first_order_datetime'] = dd.to_datetime(df['first_order_datetime'])\n",
    "    df[['country_code', 'collect_type', 'payment_method']] = df[['country_code', 'collect_type', 'payment_method']].astype('category')\n",
    "    return df\n",
    "\n",
    "df = convert_dtypes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongj\\env\\Lib\\site-packages\\dask_expr\\_collection.py:4196: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('payment_method', 'category'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "# Payment method grouping\n",
    "def group_payment_methods(payment_method):\n",
    "    mapping = {\n",
    "        'CreditCard': ['GenericCreditCard', 'CybersourceCreditCard', 'CybersourceApplePay', 'CreditCard'],\n",
    "        'DigitalWallet': ['GCash', 'AFbKash', 'JazzCashWallet', 'AdyenBoost', 'PayPal'],\n",
    "        'BankTransfer': ['XenditDirectDebit', 'RazerOnlineBanking'],\n",
    "        'PaymentOnDelivery': ['Invoice', 'PayOnDelivery']\n",
    "    }\n",
    "    for key, values in mapping.items():\n",
    "        if payment_method in values:\n",
    "            return key\n",
    "    return 'Others'\n",
    "\n",
    "df['payment_method'] = df['payment_method'].map(group_payment_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date transformations\n",
    "def date_transformations(df):\n",
    "    df['days_since_first_order'] = (df['order_date'] - df['first_order_datetime']).dt.days\n",
    "    df = df.drop(columns=['first_order_datetime'])\n",
    "    df['order_date_day_of_week'] = df['order_date'].dt.dayofweek\n",
    "    df['order_date_day'] = df['order_date'].dt.day\n",
    "    df['order_date_month'] = df['order_date'].dt.month\n",
    "    df['order_date_year'] = df['order_date'].dt.year\n",
    "    df = df.drop(columns=['order_date'])\n",
    "    return df\n",
    "\n",
    "df = date_transformations(df)\n",
    "df = df.drop(columns=['order_id', 'customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongj\\env\\Lib\\site-packages\\dask_ml\\model_selection\\_split.py:464: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X = df.drop(columns=['is_fraud'])\n",
    "y = df['is_fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment configurations\n",
    "search_space = {\n",
    "    'scaler': [None, StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "    'encode': [{'apply': True, 'columns': ['categorical_col']}],\n",
    "    'models': [\n",
    "        # {'name': 'LogisticRegression', 'instance': LogisticRegression()},\n",
    "        # {'name': 'RandomForest', 'instance': RandomForestClassifier()},\n",
    "        # {'name': 'LightGBM', 'instance': LGBMClassifier()},\n",
    "        # {'name': 'GaussianNB', 'instance': GaussianNB()},\n",
    "        # {'name': 'DecisionTree', 'instance': DecisionTreeClassifier()},\n",
    "        # {'name': 'GradientBoosting', 'instance': GradientBoostingClassifier()},\n",
    "        # {'name': 'CatBoostClassifier', 'instance': CatBoostClassifier()},\n",
    "        # xg boost\n",
    "        {'name': 'XGBoost', 'instance': XGBClassifier()},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate all combinations\n",
    "keys, values = zip(*search_space.items())\n",
    "experiment_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "categorical_cols = ['payment_method', 'country_code', 'collect_type']\n",
    "numeric_columns = ['order_value', 'refund_value', 'num_items_ordered', 'days_since_first_order',\n",
    "                   'order_date_day_of_week', 'order_date_day', 'order_date_month', 'order_date_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(experiment_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'scaler': None, 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'XGBoost', 'instance': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)}}, {'scaler': StandardScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'XGBoost', 'instance': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)}}, {'scaler': MinMaxScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'XGBoost', 'instance': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)}}, {'scaler': RobustScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'XGBoost', 'instance': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)}}] (<dask_expr.expr.Scalar: expr=(SplitTake(frame=Split(frame=Drop(frame=Drop(frame=Drop(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Drop(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=ReadCSV(a74e046))))))))), columns=['first_order_datetime']))))), columns=['order_date']), columns=['order_id', 'customer_id']), columns=['is_fraud']), frac=[0.8, 0.2], random_state=1608637542, shuffle=False), i=0, ndim=2)).size() // 17, dtype=int64>, 17) (<dask_expr.expr.Scalar: expr=(SplitTake(frame=Split(frame=(Drop(frame=Drop(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Drop(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=ReadCSV(a74e046))))))))), columns=['first_order_datetime']))))), columns=['order_date']), columns=['order_id', 'customer_id']))['is_fraud'], frac=[0.8, 0.2], random_state=1608637542, shuffle=False), i=0, ndim=1)).size(), dtype=int64>,) (<dask_expr.expr.Scalar: expr=(SplitTake(frame=Split(frame=Drop(frame=Drop(frame=Drop(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Drop(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=ReadCSV(a74e046))))))))), columns=['first_order_datetime']))))), columns=['order_date']), columns=['order_id', 'customer_id']), columns=['is_fraud']), frac=[0.8, 0.2], random_state=1608637542, shuffle=False), i=1, ndim=2)).size() // 17, dtype=int64>, 17) (<dask_expr.expr.Scalar: expr=(SplitTake(frame=Split(frame=(Drop(frame=Drop(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Drop(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=ReadCSV(a74e046))))))))), columns=['first_order_datetime']))))), columns=['order_date']), columns=['order_id', 'customer_id']))['is_fraud'], frac=[0.8, 0.2], random_state=1608637542, shuffle=False), i=1, ndim=1)).size(), dtype=int64>,) ['order_value', 'refund_value', 'num_items_ordered', 'days_since_first_order', 'order_date_day_of_week', 'order_date_day', 'order_date_month', 'order_date_year'] ['payment_method', 'country_code', 'collect_type']\n"
     ]
    }
   ],
   "source": [
    "print(experiment_combinations,X_train.shape,y_train.shape,X_test.shape,y_test.shape,numeric_columns,categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ExperimentTracker2 import PhaseOneExperimentTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Client(local_directory=\"dask_tmp\")\n",
    "\n",
    "# X_train_future = client.scatter(X_train, broadcast=True)\n",
    "# y_train_future = client.scatter(y_train, broadcast=True)\n",
    "# X_test_future = client.scatter(X_test, broadcast=True)\n",
    "# y_test_future = client.scatter(y_test, broadcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 56675 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run: XG_NoneType_Enc_202502132111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongj\\OneDrive - Singapore Polytechnic\\SP DAAA-Y2\\Y2S2\\PAI\\PAI_CA2\\ExperimentTracker2.py:156: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  pipeline.fit(X_train.compute(), y_train.compute().ravel())\n",
      "c:\\Users\\yongj\\env\\Lib\\site-packages\\mlflow\\types\\utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/02/13 21:13:58 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: MlflowException(\"Failed to enforce schema of data '  country_code  mobile_verified  num_orders_last_50days  \\\\\\n0           BD             True                       0   \\n\\n   num_cancelled_orders_last_50days  num_refund_orders_last_50days  \\\\\\n0                                 0                              0   \\n\\n   total_payment_last_50days  num_associated_customers collect_type  \\\\\\n0                        0.0                         3     delivery   \\n\\n      payment_method  order_value  num_items_ordered  refund_value  \\\\\\n0  PaymentOnDelivery     8.664062                  9      0.870117   \\n\\n   days_since_first_order  order_date_day_of_week  order_date_day  \\\\\\n0                     237                       5               8   \\n\\n   order_date_month  order_date_year  \\n0                 4             2023  ' with schema '['country_code': string (required), 'mobile_verified': boolean (required), 'num_orders_last_50days': long (required), 'num_cancelled_orders_last_50days': long (required), 'num_refund_orders_last_50days': long (required), 'total_payment_last_50days': double (required), 'num_associated_customers': long (required), 'collect_type': string (required), 'payment_method': string (required), 'order_value': float (required), 'num_items_ordered': integer (required), 'refund_value': float (required), 'days_since_first_order': long (required), 'order_date_day_of_week': integer (required), 'order_date_day': integer (required), 'order_date_month': integer (required), 'order_date_year': integer (required)]'. Error: Incompatible input types for column country_code. Can not safely convert category to <U0.\"). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847e508ec0774382b97f8e42308e5eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run: XG_NoneType_Enc_202502132111\n",
      "🏃 View run XG_NoneType_Enc_202502132111 at: https://dagshub.com/REHXZ/PAI_CA2.mlflow/#/experiments/18/runs/f4ba821503dc4f89a33a859f0e88b0c6\n",
      "🧪 View experiment at: https://dagshub.com/REHXZ/PAI_CA2.mlflow/#/experiments/18\n",
      "end run\n",
      "Starting run: XG_Standard_Enc_202502132119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongj\\OneDrive - Singapore Polytechnic\\SP DAAA-Y2\\Y2S2\\PAI\\PAI_CA2\\ExperimentTracker2.py:156: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  pipeline.fit(X_train.compute(), y_train.compute().ravel())\n",
      "c:\\Users\\yongj\\env\\Lib\\site-packages\\mlflow\\types\\utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/02/13 21:21:33 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: MlflowException(\"Failed to enforce schema of data '  country_code  mobile_verified  num_orders_last_50days  \\\\\\n0           BD             True                       0   \\n\\n   num_cancelled_orders_last_50days  num_refund_orders_last_50days  \\\\\\n0                                 0                              0   \\n\\n   total_payment_last_50days  num_associated_customers collect_type  \\\\\\n0                        0.0                         3     delivery   \\n\\n      payment_method  order_value  num_items_ordered  refund_value  \\\\\\n0  PaymentOnDelivery     8.664062                  9      0.870117   \\n\\n   days_since_first_order  order_date_day_of_week  order_date_day  \\\\\\n0                     237                       5               8   \\n\\n   order_date_month  order_date_year  \\n0                 4             2023  ' with schema '['country_code': string (required), 'mobile_verified': boolean (required), 'num_orders_last_50days': long (required), 'num_cancelled_orders_last_50days': long (required), 'num_refund_orders_last_50days': long (required), 'total_payment_last_50days': double (required), 'num_associated_customers': long (required), 'collect_type': string (required), 'payment_method': string (required), 'order_value': float (required), 'num_items_ordered': integer (required), 'refund_value': float (required), 'days_since_first_order': long (required), 'order_date_day_of_week': integer (required), 'order_date_day': integer (required), 'order_date_month': integer (required), 'order_date_year': integer (required)]'. Error: Incompatible input types for column country_code. Can not safely convert category to <U0.\"). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3e07e00291481a8f992bf3834cc1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 21:22:16,675 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:56700'.\n",
      "2025-02-13 21:22:36,409 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run: XG_Standard_Enc_202502132119\n",
      "🏃 View run XG_Standard_Enc_202502132119 at: https://dagshub.com/REHXZ/PAI_CA2.mlflow/#/experiments/18/runs/e0e6c8b301df4eefac333b775077fc62\n",
      "🧪 View experiment at: https://dagshub.com/REHXZ/PAI_CA2.mlflow/#/experiments/18\n",
      "end run\n",
      "Starting run: XG_MinMax_Enc_202502132126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongj\\OneDrive - Singapore Polytechnic\\SP DAAA-Y2\\Y2S2\\PAI\\PAI_CA2\\ExperimentTracker2.py:156: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  pipeline.fit(X_train.compute(), y_train.compute().ravel())\n",
      "c:\\Users\\yongj\\env\\Lib\\site-packages\\mlflow\\types\\utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/02/13 21:28:59 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: MlflowException(\"Failed to enforce schema of data '  country_code  mobile_verified  num_orders_last_50days  \\\\\\n0           BD             True                       0   \\n\\n   num_cancelled_orders_last_50days  num_refund_orders_last_50days  \\\\\\n0                                 0                              0   \\n\\n   total_payment_last_50days  num_associated_customers collect_type  \\\\\\n0                        0.0                         3     delivery   \\n\\n      payment_method  order_value  num_items_ordered  refund_value  \\\\\\n0  PaymentOnDelivery     8.664062                  9      0.870117   \\n\\n   days_since_first_order  order_date_day_of_week  order_date_day  \\\\\\n0                     237                       5               8   \\n\\n   order_date_month  order_date_year  \\n0                 4             2023  ' with schema '['country_code': string (required), 'mobile_verified': boolean (required), 'num_orders_last_50days': long (required), 'num_cancelled_orders_last_50days': long (required), 'num_refund_orders_last_50days': long (required), 'total_payment_last_50days': double (required), 'num_associated_customers': long (required), 'collect_type': string (required), 'payment_method': string (required), 'order_value': float (required), 'num_items_ordered': integer (required), 'refund_value': float (required), 'days_since_first_order': long (required), 'order_date_day_of_week': integer (required), 'order_date_day': integer (required), 'order_date_month': integer (required), 'order_date_year': integer (required)]'. Error: Incompatible input types for column country_code. Can not safely convert category to <U0.\"). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8f45d1b3f540708379903cb0dc704e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 21:31:35,055 - distributed.scheduler - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\compatibility.py\", line 204, in asyncio_run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 640, in run_until_complete\n",
      "    self.run_forever()\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1884, in _run_once\n",
      "    event_list = self._selector.select(timeout)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py\", line 323, in select\n",
      "    r, w, _ = self._select(self._readers, self._writers, [], timeout)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py\", line 314, in _select\n",
      "    r, w, x = select.select(r, w, w, timeout)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\utils.py\", line 805, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\scheduler.py\", line 4593, in add_worker\n",
      "    await self.handle_worker(comm, address)\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\scheduler.py\", line 6168, in handle_worker\n",
      "    await self.handle_stream(comm=comm, extra={\"worker\": worker})\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\core.py\", line 889, in handle_stream\n",
      "    msgs = await comm.read()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\comm\\tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "asyncio.exceptions.CancelledError\n",
      "2025-02-13 21:31:35,193 - distributed.scheduler - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\compatibility.py\", line 204, in asyncio_run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 640, in run_until_complete\n",
      "    self.run_forever()\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1884, in _run_once\n",
      "    event_list = self._selector.select(timeout)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py\", line 323, in select\n",
      "    r, w, _ = self._select(self._readers, self._writers, [], timeout)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py\", line 314, in _select\n",
      "    r, w, x = select.select(r, w, w, timeout)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\utils.py\", line 805, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\scheduler.py\", line 4593, in add_worker\n",
      "    await self.handle_worker(comm, address)\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\scheduler.py\", line 6168, in handle_worker\n",
      "    await self.handle_stream(comm=comm, extra={\"worker\": worker})\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\core.py\", line 889, in handle_stream\n",
      "    msgs = await comm.read()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\comm\\tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "asyncio.exceptions.CancelledError\n",
      "2025-02-13 21:31:35,209 - distributed.scheduler - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\compatibility.py\", line 204, in asyncio_run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 640, in run_until_complete\n",
      "    self.run_forever()\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1884, in _run_once\n",
      "    event_list = self._selector.select(timeout)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py\", line 323, in select\n",
      "    r, w, _ = self._select(self._readers, self._writers, [], timeout)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yongj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\selectors.py\", line 314, in _select\n",
      "    r, w, x = select.select(r, w, w, timeout)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\utils.py\", line 805, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\scheduler.py\", line 4593, in add_worker\n",
      "    await self.handle_worker(comm, address)\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\scheduler.py\", line 6168, in handle_worker\n",
      "    await self.handle_stream(comm=comm, extra={\"worker\": worker})\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\core.py\", line 889, in handle_stream\n",
      "    msgs = await comm.read()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\comm\\tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "asyncio.exceptions.CancelledError\n",
      "2025-02-13 21:31:35,526 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:56699'.\n",
      "2025-02-13 21:31:35,737 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:56707'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run XG_MinMax_Enc_202502132126 at: https://dagshub.com/REHXZ/PAI_CA2.mlflow/#/experiments/18/runs/03b028a30c6a4aebb49993b519d00e14\n",
      "🧪 View experiment at: https://dagshub.com/REHXZ/PAI_CA2.mlflow/#/experiments/18\n",
      "Error in run XG_MinMax_Enc_202502132126: Could not pickle the task to send it to the workers.\n",
      "end run\n",
      "Starting run: XG_Robust_Enc_202502132131\n",
      "🏃 View run XG_Robust_Enc_202502132131 at: https://dagshub.com/REHXZ/PAI_CA2.mlflow/#/experiments/18/runs/88a1423f45484ee68c906e33bac5883c\n",
      "🧪 View experiment at: https://dagshub.com/REHXZ/PAI_CA2.mlflow/#/experiments/18\n",
      "Error in run XG_Robust_Enc_202502132131: IOLoop is closed\n",
      "end run\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "IOLoop is closed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize tracker and run experiments\u001b[39;00m\n\u001b[0;32m      2\u001b[0m tracker \u001b[38;5;241m=\u001b[39m PhaseOneExperimentTracker(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhase-1 (Final)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_combinations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_cols\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yongj\\OneDrive - Singapore Polytechnic\\SP DAAA-Y2\\Y2S2\\PAI\\PAI_CA2\\ExperimentTracker2.py:203\u001b[0m, in \u001b[0;36mPhaseOneExperimentTracker.run_experiments\u001b[1;34m(self, experiment_combinations, X_train, y_train, X_test, y_test, numeric_columns, categorical_cols)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend run\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m         mlflow\u001b[38;5;241m.\u001b[39mend_run()  \u001b[38;5;66;03m# Ensure the run is properly ended\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\client.py:1997\u001b[0m, in \u001b[0;36mClient.close\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1994\u001b[0m         coro \u001b[38;5;241m=\u001b[39m wait_for(coro, timeout)\n\u001b[0;32m   1995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m coro\n\u001b[1;32m-> 1997\u001b[0m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_close\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1998\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_finalizing():\n",
      "File \u001b[1;32mc:\\Users\\yongj\\env\\Lib\\site-packages\\distributed\\utils.py:392\u001b[0m, in \u001b[0;36msync\u001b[1;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    390\u001b[0m timeout \u001b[38;5;241m=\u001b[39m _parse_timedelta(callback_timeout, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop\u001b[38;5;241m.\u001b[39masyncio_loop\u001b[38;5;241m.\u001b[39mis_closed():  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOLoop is closed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    394\u001b[0m e \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mEvent()\n\u001b[0;32m    395\u001b[0m main_tid \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mget_ident()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: IOLoop is closed"
     ]
    }
   ],
   "source": [
    "# Initialize tracker and run experiments\n",
    "tracker = PhaseOneExperimentTracker(\"Phase-1 (Final)\")\n",
    "tracker.run_experiments(experiment_combinations, X_train, y_train, X_test, y_test, numeric_columns, categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "X_train = dd.read_csv('./data/X_train.csv')\n",
    "y_train = dd.read_csv('./data/y_train.csv')\n",
    "\n",
    "X_test = dd.read_csv('./data/X_test.csv')\n",
    "y_test = dd.read_csv('./data/y_test.csv')\n",
    "\n",
    "X_train_ISO = dd.read_csv('./data/X_train_ISO.csv')\n",
    "y_train_ISO = dd.read_csv('./data/y_train_ISO.csv')\n",
    "\n",
    "X_train_ISO_SMOTE = dd.read_csv('./data/X_train_ISO_smote.csv')\n",
    "y_train_ISO_SMOTE = dd.read_csv('./data/y_train_ISO_smote.csv')\n",
    "\n",
    "X_train_ISO_ROS = dd.read_csv('./data/X_train_ISO_ros.csv')\n",
    "y_train_ISO_ROS = dd.read_csv('./data/y_train_ISO_ros.csv')\n",
    "\n",
    "X_train_ISO_RUS = dd.read_csv('./data/X_train_ISO_rus.csv')\n",
    "y_train_ISO_RUS = dd.read_csv('./data/y_train_ISO_rus.csv')\n",
    "\n",
    "X_train_LOF = dd.read_csv('./data/X_train_LOF.csv')\n",
    "y_train_LOF = dd.read_csv('./data/y_train_LOF.csv')\n",
    "\n",
    "X_train_LOF_SMOTE = dd.read_csv('./data/X_train_LOF_smote.csv')\n",
    "y_train_LOF_SMOTE = dd.read_csv('./data/y_train_LOF_smote.csv')\n",
    "\n",
    "X_train_LOF_ROS = dd.read_csv('./data/X_train_LOF_ros.csv')\n",
    "y_train_LOF_ROS = dd.read_csv('./data/y_train_LOF_ros.csv')\n",
    "\n",
    "X_train_LOF_RUS = dd.read_csv('./data/X_train_LOF_rus.csv')\n",
    "y_train_LOF_RUS = dd.read_csv('./data/y_train_LOF_rus.csv')\n",
    "\n",
    "X_train_smote = dd.read_csv('./data/X_train_smote.csv')\n",
    "y_train_smote = dd.read_csv('./data/y_train_smote.csv')\n",
    "\n",
    "X_train_ros = dd.read_csv('./data/X_train_ros.csv')\n",
    "y_train_ros = dd.read_csv('./data/y_train_ros.csv')\n",
    "\n",
    "X_train_rus = dd.read_csv('./data/X_train_rus.csv')\n",
    "y_train_rus = dd.read_csv('./data/y_train_rus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    (\"dataset_default\", X_train, y_train),\n",
    "    (\"dataset_ISO\", X_train_ISO, y_train_ISO),\n",
    "    (\"dataset_ISO_SMOTE\", X_train_ISO_SMOTE, y_train_ISO_SMOTE),\n",
    "    (\"dataset_ISO_ROS\", X_train_ISO_ROS, y_train_ISO_ROS),\n",
    "    (\"dataset_ISO_RUS\", X_train_ISO_RUS, y_train_ISO_RUS),\n",
    "    (\"dataset_LOF\", X_train_LOF, y_train_LOF),\n",
    "    (\"dataset_LOF_SMOTE\", X_train_LOF_SMOTE, y_train_LOF_SMOTE),\n",
    "    (\"dataset_LOF_ROS\", X_train_LOF_ROS, y_train_LOF_ROS),\n",
    "    (\"dataset_LOF_RUS\", X_train_LOF_RUS, y_train_LOF_RUS),\n",
    "    (\"dataset_SMOTE\", X_train_smote, y_train_smote),\n",
    "    (\"dataset_ROS\", X_train_ros, y_train_ros),\n",
    "    (\"dataset_RUS\", X_train_rus, y_train_rus)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment configurations\n",
    "search_space = {\n",
    "    'scaler': [None],\n",
    "    'encode': [{'apply': True, 'columns': ['categorical_col']}],\n",
    "    'models': [\n",
    "        {'name': 'LogisticRegression', 'instance': LogisticRegression()},\n",
    "        {'name': 'RandomForest', 'instance': RandomForestClassifier()},\n",
    "        {'name': 'LightGBM', 'instance': LGBMClassifier()},\n",
    "        {'name': 'GaussianNB', 'instance': GaussianNB()},\n",
    "        {'name': 'DecisionTree', 'instance': DecisionTreeClassifier()},\n",
    "        {'name': 'GradientBoosting', 'instance': GradientBoostingClassifier()},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate all combinations\n",
    "keys, values = zip(*search_space.items())\n",
    "experiment_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "categorical_cols = ['payment_method', 'country_code', 'collect_type']\n",
    "numeric_columns = ['order_value', 'refund_value', 'num_items_ordered', 'days_since_first_order',\n",
    "                   'order_date_day_of_week', 'order_date_day', 'order_date_month', 'order_date_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the tracker\n",
    "# from ExperimentTracker2 import PhaseTwoExperimentTracker\n",
    "# tracker = PhaseTwoExperimentTracker(\"Phase-2 (Final)\")\n",
    "\n",
    "# # Load checkpoint file\n",
    "# tracker.completed_runs\n",
    "\n",
    "# # Run experiments with checkpointing\n",
    "# tracker.run_experiments(\n",
    "#     datasets=datasets,\n",
    "#     experiment_combinations=experiment_combinations,\n",
    "#     X_test=X_test,\n",
    "#     y_test=y_test,\n",
    "#     numeric_columns=numeric_columns,\n",
    "#     categorical_cols=categorical_cols\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_experiment_combinations = [\n",
    "    {\n",
    "        \"scaler\": MinMaxScaler(),\n",
    "        \"encode\": {\"apply\": True, \"columns\": [\"categorical_col\"]},\n",
    "        \"models\": {\"name\": \"RandomForest\", \"instance\": RandomForestClassifier()}\n",
    "    },\n",
    "    {\n",
    "        \"scaler\": None,\n",
    "        \"encode\": {\"apply\": True, \"columns\": [\"categorical_col\"]},\n",
    "        \"models\": {\"name\": \"RandomForest\", \"instance\": RandomForestClassifier()}\n",
    "    },\n",
    "    # {\n",
    "    #     \"scaler\": StandardScaler(),\n",
    "    #     \"encode\": {\"apply\": True, \"columns\": [\"categorical_col\"]},\n",
    "    #     \"models\": {\"name\": \"LightGBM\", \"instance\": LGBMClassifier()}\n",
    "    # },\n",
    "    # {\n",
    "    #     \"scaler\": None,\n",
    "    #     \"encode\": {\"apply\": True, \"columns\": [\"categorical_col\"]},\n",
    "    #     \"models\": {\"name\": \"LightGBM\", \"instance\": LGBMClassifier()}\n",
    "    # },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ExperimentTracker2 import PhaseThreeExperimentTracker\n",
    "\n",
    "# tracker = PhaseThreeExperimentTracker(\"Models with Scaler\")\n",
    "\n",
    "# # Load checkpoint file\n",
    "# tracker.completed_runs\n",
    "\n",
    "# # Run experiments with checkpointing\n",
    "# tracker.run_experiments(\n",
    "#     datasets=datasets,\n",
    "#     experiment_combinations=defined_experiment_combinations,\n",
    "#     X_test=X_test,\n",
    "#     y_test=y_test,\n",
    "#     numeric_columns=numeric_columns,\n",
    "#     categorical_cols=categorical_cols\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PAIenv (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
