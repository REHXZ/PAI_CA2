{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "from ExperimentTracker2 import PhaseOneExperimentTracker\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from dask_ml.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load large dataset with Dask\n",
    "df = dd.read_csv(\"merged_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types for memory efficiency\n",
    "def convert_dtypes(df):\n",
    "    df['order_value'] = df['order_value'].astype('float32')\n",
    "    df['refund_value'] = df['refund_value'].astype('float32')\n",
    "    df['num_items_ordered'] = df['num_items_ordered'].astype(float).round().astype('uint8')\n",
    "    df['order_date'] = dd.to_datetime(df['order_date'])\n",
    "    df['first_order_datetime'] = dd.to_datetime(df['first_order_datetime'])\n",
    "    df[['country_code', 'collect_type', 'payment_method']] = df[['country_code', 'collect_type', 'payment_method']].astype('category')\n",
    "    return df\n",
    "\n",
    "df = convert_dtypes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment method grouping\n",
    "def group_payment_methods(payment_method):\n",
    "    mapping = {\n",
    "        'CreditCard': ['GenericCreditCard', 'CybersourceCreditCard', 'CybersourceApplePay', 'CreditCard'],\n",
    "        'DigitalWallet': ['GCash', 'AFbKash', 'JazzCashWallet', 'AdyenBoost', 'PayPal'],\n",
    "        'BankTransfer': ['XenditDirectDebit', 'RazerOnlineBanking'],\n",
    "        'PaymentOnDelivery': ['Invoice', 'PayOnDelivery']\n",
    "    }\n",
    "    for key, values in mapping.items():\n",
    "        if payment_method in values:\n",
    "            return key\n",
    "    return 'Others'\n",
    "\n",
    "df['payment_method'] = df['payment_method'].map(group_payment_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date transformations\n",
    "def date_transformations(df):\n",
    "    df['days_since_first_order'] = (df['order_date'] - df['first_order_datetime']).dt.days\n",
    "    df = df.drop(columns=['first_order_datetime'])\n",
    "    df['order_date_day_of_week'] = df['order_date'].dt.dayofweek\n",
    "    df['order_date_day'] = df['order_date'].dt.day\n",
    "    df['order_date_month'] = df['order_date'].dt.month\n",
    "    df['order_date_year'] = df['order_date'].dt.year\n",
    "    df = df.drop(columns=['order_date'])\n",
    "    return df\n",
    "\n",
    "df = date_transformations(df)\n",
    "df = df.drop(columns=['order_id', 'customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\REHXX\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\dask_ml\\model_selection\\_split.py:464: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X = df.drop(columns=['is_fraud'])\n",
    "y = df['is_fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment configurations\n",
    "search_space = {\n",
    "    'scaler': [None, StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "    'encode': [{'apply': True, 'columns': ['categorical_col']}, {'apply': False}],\n",
    "    'models': [\n",
    "        {'name': 'LogisticRegression', 'instance': LogisticRegression()},\n",
    "        {'name': 'RandomForest', 'instance': RandomForestClassifier()},\n",
    "        {'name': 'LightGBM', 'instance': LGBMClassifier()},\n",
    "        {'name': 'GaussianNB', 'instance': GaussianNB()},\n",
    "        {'name': 'DecisionTree', 'instance': DecisionTreeClassifier()},\n",
    "        {'name': 'GradientBoosting', 'instance': GradientBoostingClassifier()},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate all combinations\n",
    "keys, values = zip(*search_space.items())\n",
    "experiment_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "categorical_cols = ['payment_method', 'country_code', 'collect_type']\n",
    "numeric_columns = ['order_value', 'refund_value', 'num_items_ordered', 'days_since_first_order',\n",
    "                   'order_date_day_of_week', 'order_date_day', 'order_date_month', 'order_date_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(experiment_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'scaler': None, 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'LogisticRegression', 'instance': LogisticRegression()}}, {'scaler': None, 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'RandomForest', 'instance': RandomForestClassifier()}}, {'scaler': None, 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'LightGBM', 'instance': LGBMClassifier()}}, {'scaler': None, 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'GaussianNB', 'instance': GaussianNB()}}, {'scaler': None, 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'DecisionTree', 'instance': DecisionTreeClassifier()}}, {'scaler': None, 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'GradientBoosting', 'instance': GradientBoostingClassifier()}}, {'scaler': StandardScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'LogisticRegression', 'instance': LogisticRegression()}}, {'scaler': StandardScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'RandomForest', 'instance': RandomForestClassifier()}}, {'scaler': StandardScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'LightGBM', 'instance': LGBMClassifier()}}, {'scaler': StandardScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'GaussianNB', 'instance': GaussianNB()}}, {'scaler': StandardScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'DecisionTree', 'instance': DecisionTreeClassifier()}}, {'scaler': StandardScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'GradientBoosting', 'instance': GradientBoostingClassifier()}}, {'scaler': MinMaxScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'LogisticRegression', 'instance': LogisticRegression()}}, {'scaler': MinMaxScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'RandomForest', 'instance': RandomForestClassifier()}}, {'scaler': MinMaxScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'LightGBM', 'instance': LGBMClassifier()}}, {'scaler': MinMaxScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'GaussianNB', 'instance': GaussianNB()}}, {'scaler': MinMaxScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'DecisionTree', 'instance': DecisionTreeClassifier()}}, {'scaler': MinMaxScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'GradientBoosting', 'instance': GradientBoostingClassifier()}}, {'scaler': RobustScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'LogisticRegression', 'instance': LogisticRegression()}}, {'scaler': RobustScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'RandomForest', 'instance': RandomForestClassifier()}}, {'scaler': RobustScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'LightGBM', 'instance': LGBMClassifier()}}, {'scaler': RobustScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'GaussianNB', 'instance': GaussianNB()}}, {'scaler': RobustScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'DecisionTree', 'instance': DecisionTreeClassifier()}}, {'scaler': RobustScaler(), 'encode': {'apply': True, 'columns': ['categorical_col']}, 'models': {'name': 'GradientBoosting', 'instance': GradientBoostingClassifier()}}] (Delayed('int-e0898709-e3f4-4145-b47e-a3cf1e4d5cfc'), 17) (dd.Scalar<size-ag..., dtype=int32>,) (Delayed('int-2fc7c3e6-db59-40eb-997c-92bbbda7eb96'), 17) (dd.Scalar<size-ag..., dtype=int32>,) ['order_value', 'refund_value', 'num_items_ordered', 'days_since_first_order', 'order_date_day_of_week', 'order_date_day', 'order_date_month', 'order_date_year'] ['payment_method', 'country_code', 'collect_type']\n"
     ]
    }
   ],
   "source": [
    "print(experiment_combinations,X_train.shape,y_train.shape,X_test.shape,y_test.shape,numeric_columns,categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ExperimentTracker2 import PhaseOneExperimentTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Client(local_directory=\"dask_tmp\")\n",
    "\n",
    "# X_train_future = client.scatter(X_train, broadcast=True)\n",
    "# y_train_future = client.scatter(y_train, broadcast=True)\n",
    "# X_test_future = client.scatter(X_test, broadcast=True)\n",
    "# y_test_future = client.scatter(y_test, broadcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize tracker and run experiments\n",
    "# tracker = PhaseOneExperimentTracker(\"Phase-1 (Final)\")\n",
    "# tracker.run_experiments(experiment_combinations, X_train, y_train, X_test, y_test, numeric_columns, categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "X_train = dd.read_csv('./data/X_train.csv')\n",
    "y_train = dd.read_csv('./data/y_train.csv')\n",
    "\n",
    "X_test = dd.read_csv('./data/X_test.csv')\n",
    "y_test = dd.read_csv('./data/y_test.csv')\n",
    "\n",
    "X_train_LOF_ROS = dd.read_csv('./data/X_train_LOF_ros.csv')\n",
    "y_train_LOF_ROS = dd.read_csv('./data/y_train_LOF_ros.csv')\n",
    "\n",
    "X_train_LOF = dd.read_csv('./data/X_train_LOF.csv')\n",
    "y_train_LOF = dd.read_csv('./data/y_train_LOF.csv')\n",
    "\n",
    "datasets = [\n",
    "    (\"dataset_default\", X_train, y_train),\n",
    "    (\"dataset_LOF\", X_train_LOF, y_train_LOF),\n",
    "    (\"dataset_LOF_ROS\", X_train_LOF_ROS, y_train_LOF_ROS),\n",
    "]\n",
    "\n",
    "# X_train_ISO = dd.read_csv('./data/X_train_ISO.csv')\n",
    "# y_train_ISO = dd.read_csv('./data/y_train_ISO.csv')\n",
    "\n",
    "# X_train_ISO_SMOTE = dd.read_csv('./data/X_train_ISO_smote.csv')\n",
    "# y_train_ISO_SMOTE = dd.read_csv('./data/y_train_ISO_smote.csv')\n",
    "\n",
    "# X_train_ISO_ROS = dd.read_csv('./data/X_train_ISO_ros.csv')\n",
    "# y_train_ISO_ROS = dd.read_csv('./data/y_train_ISO_ros.csv')\n",
    "\n",
    "# X_train_ISO_RUS = dd.read_csv('./data/X_train_ISO_rus.csv')\n",
    "# y_train_ISO_RUS = dd.read_csv('./data/y_train_ISO_rus.csv')\n",
    "\n",
    "\n",
    "# X_train_LOF_SMOTE = dd.read_csv('./data/X_train_LOF_smote.csv')\n",
    "# y_train_LOF_SMOTE = dd.read_csv('./data/y_train_LOF_smote.csv')\n",
    "\n",
    "\n",
    "# X_train_LOF_RUS = dd.read_csv('./data/X_train_LOF_rus.csv')\n",
    "# y_train_LOF_RUS = dd.read_csv('./data/y_train_LOF_rus.csv')\n",
    "\n",
    "# X_train_smote = dd.read_csv('./data/X_train_smote.csv')\n",
    "# y_train_smote = dd.read_csv('./data/y_train_smote.csv')\n",
    "\n",
    "# X_train_ros = dd.read_csv('./data/X_train_ros.csv')\n",
    "# y_train_ros = dd.read_csv('./data/y_train_ros.csv')\n",
    "\n",
    "# X_train_rus = dd.read_csv('./data/X_train_rus.csv')\n",
    "# y_train_rus = dd.read_csv('./data/y_train_rus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    (\"dataset_default\", X_train, y_train),\n",
    "    # (\"dataset_ISO\", X_train_ISO, y_train_ISO),\n",
    "    # (\"dataset_ISO_SMOTE\", X_train_ISO_SMOTE, y_train_ISO_SMOTE),\n",
    "    # (\"dataset_ISO_ROS\", X_train_ISO_ROS, y_train_ISO_ROS),\n",
    "    # (\"dataset_ISO_RUS\", X_train_ISO_RUS, y_train_ISO_RUS),\n",
    "    (\"dataset_LOF\", X_train_LOF, y_train_LOF),\n",
    "    # (\"dataset_LOF_SMOTE\", X_train_LOF_SMOTE, y_train_LOF_SMOTE),\n",
    "    (\"dataset_LOF_ROS\", X_train_LOF_ROS, y_train_LOF_ROS),\n",
    "    # (\"dataset_LOF_RUS\", X_train_LOF_RUS, y_train_LOF_RUS),\n",
    "    # (\"dataset_SMOTE\", X_train_smote, y_train_smote),\n",
    "    # (\"dataset_ROS\", X_train_ros, y_train_ros),\n",
    "    # (\"dataset_RUS\", X_train_rus, y_train_rus)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment configurations\n",
    "search_space = {\n",
    "    'scaler': [None],\n",
    "    'encode': [{'apply': True, 'columns': ['categorical_col']}],\n",
    "    'models': [\n",
    "        {'name': 'LogisticRegression', 'instance': LogisticRegression()},\n",
    "        {'name': 'RandomForest', 'instance': RandomForestClassifier()},\n",
    "        {'name': 'LightGBM', 'instance': LGBMClassifier()},\n",
    "        {'name': 'GaussianNB', 'instance': GaussianNB()},\n",
    "        {'name': 'DecisionTree', 'instance': DecisionTreeClassifier()},\n",
    "        {'name': 'GradientBoosting', 'instance': GradientBoostingClassifier()},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate all combinations\n",
    "keys, values = zip(*search_space.items())\n",
    "experiment_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "categorical_cols = ['payment_method', 'country_code', 'collect_type']\n",
    "numeric_columns = ['order_value', 'refund_value', 'num_items_ordered', 'days_since_first_order',\n",
    "                   'order_date_day_of_week', 'order_date_day', 'order_date_month', 'order_date_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the tracker\n",
    "# from ExperimentTracker2 import PhaseTwoExperimentTracker\n",
    "# tracker = PhaseTwoExperimentTracker(\"Phase-2 (Final)\")\n",
    "\n",
    "# # Load checkpoint file\n",
    "# tracker.completed_runs\n",
    "\n",
    "# # Run experiments with checkpointing\n",
    "# tracker.run_experiments(\n",
    "#     datasets=datasets,\n",
    "#     experiment_combinations=experiment_combinations,\n",
    "#     X_test=X_test,\n",
    "#     y_test=y_test,\n",
    "#     numeric_columns=numeric_columns,\n",
    "#     categorical_cols=categorical_cols\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_experiment_combinations = [\n",
    "    {\n",
    "        \"scaler\": MinMaxScaler(),\n",
    "        \"encode\": {\"apply\": True, \"columns\": [\"categorical_col\"]},\n",
    "        \"models\": {\"name\": \"RandomForest\", \"instance\": RandomForestClassifier()}\n",
    "    },\n",
    "    {\n",
    "        \"scaler\": StandardScaler(),\n",
    "        \"encode\": {\"apply\": True, \"columns\": [\"categorical_col\"]},\n",
    "        \"models\": {\"name\": \"LightGBM\", \"instance\": LGBMClassifier()}\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'scaler': None,\n",
       "  'encode': {'apply': True, 'columns': ['categorical_col']},\n",
       "  'models': {'name': 'LogisticRegression', 'instance': LogisticRegression()}},\n",
       " {'scaler': None,\n",
       "  'encode': {'apply': True, 'columns': ['categorical_col']},\n",
       "  'models': {'name': 'RandomForest', 'instance': RandomForestClassifier()}},\n",
       " {'scaler': None,\n",
       "  'encode': {'apply': True, 'columns': ['categorical_col']},\n",
       "  'models': {'name': 'LightGBM', 'instance': LGBMClassifier()}},\n",
       " {'scaler': None,\n",
       "  'encode': {'apply': True, 'columns': ['categorical_col']},\n",
       "  'models': {'name': 'GaussianNB', 'instance': GaussianNB()}},\n",
       " {'scaler': None,\n",
       "  'encode': {'apply': True, 'columns': ['categorical_col']},\n",
       "  'models': {'name': 'DecisionTree', 'instance': DecisionTreeClassifier()}},\n",
       " {'scaler': None,\n",
       "  'encode': {'apply': True, 'columns': ['categorical_col']},\n",
       "  'models': {'name': 'GradientBoosting',\n",
       "   'instance': GradientBoostingClassifier()}}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_reduced = ['country_code']\n",
    "numeric_columns_reduced = ['order_value', 'refund_value', 'num_items_ordered', 'days_since_first_order',\n",
    "                   'order_date_day_of_week', 'order_date_day', 'order_date_month', 'order_date_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ExperimentTracker2 import PhaseFourExperimentTracker\n",
    "# tracker = PhaseFourExperimentTracker(\"Models with Scaler\")\n",
    "\n",
    "# # Load checkpoint file\n",
    "# tracker.completed_runs\n",
    "\n",
    "# # Run experiments with checkpointing\n",
    "# tracker.run_experiments(\n",
    "#     datasets=datasets,\n",
    "#     experiment_combinations=defined_experiment_combinations,\n",
    "#     X_test=X_test,\n",
    "#     y_test=y_test,\n",
    "#     numeric_columns=numeric_columns_reduced,\n",
    "#     categorical_cols=categorical_cols_reduced,\n",
    "#     drop_columns=['payment_method', 'collect_type', 'mobile_verified']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_experiment_combinations = [\n",
    "    {\n",
    "        \"scaler\": MinMaxScaler(),\n",
    "        \"encode\": {\"apply\": True, \"columns\": [\"categorical_col\"]},\n",
    "        \"models\": {\"name\": \"RandomForest\", \"instance\": RandomForestClassifier()},\n",
    "        \"pca\":{\"apply\":True, \"n_components\":0.95}\n",
    "    },\n",
    "    {\n",
    "        \"scaler\": StandardScaler(),\n",
    "        \"encode\": {\"apply\": True, \"columns\": [\"categorical_col\"]},\n",
    "        \"models\": {\"name\": \"LightGBM\", \"instance\": LGBMClassifier()},\n",
    "        \"pca\":{\"apply\":True, \"n_components\":0.95}\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_reduced = ['country_code']\n",
    "numeric_columns_reduced = ['order_value', 'refund_value', 'num_items_ordered', 'days_since_first_order',\n",
    "                   'order_date_day_of_week', 'order_date_day', 'order_date_month', 'order_date_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ExperimentTracker2 import PhaseFiveExperimentTracker\n",
    "# tracker = PhaseFiveExperimentTracker(\"Models with Scaler\")\n",
    "\n",
    "# # Load checkpoint file\n",
    "# tracker.completed_runs\n",
    "\n",
    "# # Run experiments with checkpointing\n",
    "# tracker.run_experiments(\n",
    "#     datasets=datasets,\n",
    "#     experiment_combinations=defined_experiment_combinations,\n",
    "#     X_test=X_test,\n",
    "#     y_test=y_test,\n",
    "#     numeric_columns=numeric_columns_reduced,\n",
    "#     categorical_cols=categorical_cols_reduced,\n",
    "#     drop_columns=['payment_method', 'collect_type', 'mobile_verified']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_experiment_combinations = [\n",
    "    {\n",
    "        \"scaler\": MinMaxScaler(),\n",
    "        \"encode\": {\"apply\": True, \"columns\": [\"categorical_col\"]},\n",
    "        \"models\": {\"name\": \"RandomForest\", \"instance\": RandomForestClassifier()},\n",
    "        \"params\": {\n",
    "        \"model__n_estimators\": [100, 200, 300],\n",
    "        \"model__max_depth\": [10, 20, None],\n",
    "        \"model__min_samples_split\": [2, 5, 10],\n",
    "        \"model__min_samples_leaf\": [1, 2, 4]\n",
    "    }}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# categorical_cols_reduced = ['country_code']\n",
    "# numeric_columns_reduced = ['order_value', 'refund_value', 'num_items_ordered', 'days_since_first_order',\n",
    "#                    'order_date_day_of_week', 'order_date_day', 'order_date_month', 'order_date_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "\n",
    "# Initialize the Dask client\n",
    "# client = Client(dashboard_address=\":0\", local_directory=\"/tmp/dask-worker-space\", memory_limit=\"17GB\")\n",
    "\n",
    "# Load datasets\n",
    "X_train = dd.read_csv('./data/X_train.csv')\n",
    "y_train = dd.read_csv('./data/y_train.csv')\n",
    "X_test = dd.read_csv('./data/X_test.csv')\n",
    "y_test = dd.read_csv('./data/y_test.csv')\n",
    "X_train_LOF_ROS = dd.read_csv('./data/X_train_LOF_ros.csv')\n",
    "y_train_LOF_ROS = dd.read_csv('./data/y_train_LOF_ros.csv')\n",
    "X_train_LOF = dd.read_csv('./data/X_train_LOF.csv')\n",
    "y_train_LOF = dd.read_csv('./data/y_train_LOF.csv')\n",
    "\n",
    "# Update the datasets list with scattered futures\n",
    "datasets = [\n",
    "    (\"dataset_default\", X_train, y_train),\n",
    "    (\"dataset_LOF\", X_train_LOF, y_train_LOF),\n",
    "    (\"dataset_LOF_ROS\", X_train_LOF_ROS, y_train_LOF_ROS),\n",
    "]\n",
    "# from ExperimentTracker2 import PhaseSixExperimentTracker\n",
    "\n",
    "# tracker = PhaseSixExperimentTracker(\"Models with Scaler\")\n",
    "\n",
    "# tracker.completed_runs\n",
    "\n",
    "# # Pass the scattered datasets to the experiment tracker\n",
    "# tracker.run_experiments(\n",
    "#     datasets=datasets,\n",
    "#     experiment_combinations=defined_experiment_combinations,\n",
    "#     X_test=X_test,\n",
    "#     y_test=y_test,\n",
    "#     numeric_columns=numeric_columns_reduced,\n",
    "#     categorical_cols=categorical_cols_reduced,\n",
    "#     drop_columns=['payment_method', 'collect_type', 'mobile_verified'],\n",
    "#     n_iter=10\n",
    "# )\n",
    "\n",
    "# Close the Dask client after experiments are complete\n",
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run: RA_MinMax_Enc_202502140033_hypertuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\REHXX\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\distributed\\client.py:3108: UserWarning: Sending large graph of size 17.68 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "2025/02/14 00:37:34 INFO mlflow.tracking._tracking_service.client: 🏃 View run RA_MinMax_Enc_202502140033_hypertuned at: https://dagshub.com/REHXZ/PAI_CA2.mlflow/#/experiments/20/runs/061bd348d9894359843faab0353462c3.\n",
      "2025/02/14 00:37:34 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://dagshub.com/REHXZ/PAI_CA2.mlflow/#/experiments/20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end run\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m tracker\u001b[38;5;241m.\u001b[39mcompleted_runs\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Run experiments with checkpointing\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_combinations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefined_experiment_combinations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_columns_reduced\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_cols_reduced\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpayment_method\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcollect_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmobile_verified\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\REHXX\\OneDrive\\Desktop\\PAI_CA2\\ExperimentTracker2.py:589\u001b[0m, in \u001b[0;36mPhaseSixExperimentTracker.run_experiments\u001b[1;34m(self, datasets, X_test, y_test, experiment_combinations, numeric_columns, categorical_cols, drop_columns, n_iter)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;66;03m# Train the pipeline with hyperparameter tuning\u001b[39;00m\n\u001b[0;32m    588\u001b[0m y_train_series \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# Convert to Pandas Series\u001b[39;00m\n\u001b[1;32m--> 589\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_series\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m best_pipeline \u001b[38;5;241m=\u001b[39m random_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m    593\u001b[0m \u001b[38;5;66;03m# Predictions and probabilities for train and test sets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\REHXX\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\dask_ml\\model_selection\\_search.py:1272\u001b[0m, in \u001b[0;36mDaskBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m   1270\u001b[0m result_map \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1271\u001b[0m ac \u001b[38;5;241m=\u001b[39m as_completed(futures, with_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, raise_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m ac\u001b[38;5;241m.\u001b[39mbatches():\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future, result \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m   1274\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m future\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\REHXX\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\distributed\\client.py:5457\u001b[0m, in \u001b[0;36mas_completed.batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5455\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   5456\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5457\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   5458\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5459\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\REHXX\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\distributed\\client.py:5429\u001b[0m, in \u001b[0;36mas_completed.next_batch\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   5407\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the next batch of completed futures.\u001b[39;00m\n\u001b[0;32m   5408\u001b[0m \n\u001b[0;32m   5409\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5426\u001b[0m \u001b[38;5;124;03mList of futures or (future, result) tuples\u001b[39;00m\n\u001b[0;32m   5427\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m-> 5429\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   5430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5431\u001b[0m     batch \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\REHXX\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\distributed\\client.py:5390\u001b[0m, in \u001b[0;36mas_completed.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5388\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[0;32m   5389\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_condition:\n\u001b[1;32m-> 5390\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthread_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_and_raise()\n",
      "File \u001b[1;32mc:\\Users\\REHXX\\anaconda3\\envs\\gpu_env\\lib\\threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from ExperimentTracker2 import PhaseSixExperimentTracker\n",
    "# tracker = PhaseSixExperimentTracker(\"Models with Scaler\")\n",
    "\n",
    "# tracker.completed_runs\n",
    "\n",
    "# # Run experiments with checkpointing\n",
    "# tracker.run_experiments(\n",
    "#     datasets=datasets,\n",
    "#     experiment_combinations=defined_experiment_combinations,\n",
    "#     X_test=X_test,\n",
    "#     y_test=y_test,\n",
    "#     numeric_columns=numeric_columns_reduced,\n",
    "#     categorical_cols=categorical_cols_reduced,\n",
    "#     drop_columns=['payment_method', 'collect_type', 'mobile_verified'],\n",
    "#     n_iter=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_experiment_combinations = [\n",
    "    {\n",
    "        \"scaler\": StandardScaler(),\n",
    "        \"encode\": {\"apply\": True, \"columns\": [\"categorical_col\"]},\n",
    "        \"models\": {\"name\": \"LightGBM\", \"instance\": LGBMClassifier()},\n",
    "        \"params\": {\"model__learning_rate\": [0.01, 0.03, 0.05, 1], \n",
    "                    \"model__max_depth\": [3, 5, 7, 10, -1],\n",
    "                    \"model__min_samples_split\": [2, 5, 10, 20],\n",
    "                    \"model__min_samples_leaf\": [1, 5, 10, 20]}\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['payment_method', 'country_code', 'collect_type']\n",
    "numeric_columns = ['order_value', 'refund_value', 'num_items_ordered', 'days_since_first_order',\n",
    "                   'order_date_day_of_week', 'order_date_day', 'order_date_month', 'order_date_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dd.read_csv('./data/X_train.csv')\n",
    "y_train = dd.read_csv('./data/y_train.csv')\n",
    "X_test = dd.read_csv('./data/X_test.csv')\n",
    "y_test = dd.read_csv('./data/y_test.csv')\n",
    "X_train_LOF_ROS = dd.read_csv('./data/X_train_LOF_ros.csv')\n",
    "y_train_LOF_ROS = dd.read_csv('./data/y_train_LOF_ros.csv')\n",
    "X_train_LOF = dd.read_csv('./data/X_train_LOF.csv')\n",
    "y_train_LOF = dd.read_csv('./data/y_train_LOF.csv')\n",
    "\n",
    "# Update the datasets list with scattered futures\n",
    "datasets = [\n",
    "    (\"dataset_default\", X_train, y_train),\n",
    "    (\"dataset_LOF\", X_train_LOF, y_train_LOF),\n",
    "    (\"dataset_LOF_ROS\", X_train_LOF_ROS, y_train_LOF_ROS),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run: LI_Standard_Enc_202502140301_hypertuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongj\\anaconda3\\envs\\PAI_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongj\\anaconda3\\envs\\PAI_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongj\\anaconda3\\envs\\PAI_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongj\\anaconda3\\envs\\PAI_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongj\\anaconda3\\envs\\PAI_env\\lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/02/14 03:47:30 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: MlflowException(\"Failed to enforce schema of data '  country_code  mobile_verified  num_orders_last_50days  \\\\\\n0           MY             True                     205   \\n\\n   num_cancelled_orders_last_50days  num_refund_orders_last_50days  \\\\\\n0                                 7                             10   \\n\\n   total_payment_last_50days  num_associated_customers collect_type  \\\\\\n0                     1876.0                         2     delivery   \\n\\n       payment_method  order_value  num_items_ordered  refund_value  \\\\\\n0  RazerOnlineBanking        6.562                  3          6.56   \\n\\n   days_since_first_order  order_date_day_of_week  order_date_day  \\\\\\n0                    1259                       1               3   \\n\\n   order_date_month  order_date_year  \\n0                 1             2023  ' with schema '['country_code': string (required), 'mobile_verified': boolean (required), 'num_orders_last_50days': long (required), 'num_cancelled_orders_last_50days': long (required), 'num_refund_orders_last_50days': long (required), 'total_payment_last_50days': double (required), 'num_associated_customers': long (required), 'collect_type': string (required), 'payment_method': string (required), 'order_value': double (required), 'num_items_ordered': long (required), 'refund_value': double (required), 'days_since_first_order': long (required), 'order_date_day_of_week': long (required), 'order_date_day': long (required), 'order_date_month': long (required), 'order_date_year': long (required)]'. Error: Incompatible input types for column country_code. Can not safely convert string to <U0.\"). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n",
      "c:\\Users\\yongj\\anaconda3\\envs\\PAI_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] min_data_in_leaf is set with min_child_samples=20, will be overridden by min_samples_leaf=5. Current value: min_data_in_leaf=5\n",
      "Completed run: LI_Standard_Enc_202502140301_hypertuned\n",
      "🏃 View run LI_Standard_Enc_202502140301_hypertuned at: https://dagshub.com/REHXZ/PAI_CA2.mlflow/#/experiments/20/runs/32d32f553e6d46e28b8bd0898a359397\n",
      "🧪 View experiment at: https://dagshub.com/REHXZ/PAI_CA2.mlflow/#/experiments/20\n",
      "end run\n"
     ]
    }
   ],
   "source": [
    "from ExperimentTracker2 import PhaseSevenExperimentTracker\n",
    "tracker = PhaseSevenExperimentTracker(\"Models with Scaler\")\n",
    "\n",
    "tracker.completed_runs\n",
    "\n",
    "# Run experiments with checkpointing\n",
    "tracker.run_experiments(\n",
    "    datasets=datasets,\n",
    "    experiment_combinations=defined_experiment_combinations,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    numeric_columns=numeric_columns,\n",
    "    categorical_cols=categorical_cols,\n",
    "    n_iter=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PAI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
